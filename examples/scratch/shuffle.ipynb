{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "import pandas as pd \n",
    "import yaml \n",
    "import matplotlib.pyplot as plt; plt.ion()\n",
    "from matplotlib import rc, rcParams\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "config = read_config(\"/work/mccleary_group/amit.m/SMPy/SMPy/KaiserSquires/example_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shear_data(shear_cat_path, ra_col, dec_col, g1_col, g2_col, weight_col):\n",
    "    \"\"\" \n",
    "    Load shear data from a FITS file and return a pandas DataFrame.\n",
    "\n",
    "    :param path: Path to the FITS file.\n",
    "    :param ra_col: Column name for right ascension.\n",
    "    :param dec_col: Column name for declination.\n",
    "    :param g1_col: Column name for the first shear component.\n",
    "    :param g2_col: Column name for the second shear component.\n",
    "    :param weight_col: Column name for the weight.\n",
    "    :return: pandas DataFrame with the specified columns.\n",
    "    \"\"\"\n",
    "    # Read data from the FITS file\n",
    "    shear_catalog = Table.read(shear_cat_path)\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    shear_df = pd.DataFrame({\n",
    "        'ra': shear_catalog[ra_col],\n",
    "        'dec': shear_catalog[dec_col],\n",
    "        'g1': shear_catalog[g1_col],\n",
    "        'g2': shear_catalog[g2_col],\n",
    "        'weight': shear_catalog[weight_col]\n",
    "    })\n",
    "\n",
    "    return shear_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_field_boundaries(ra, dec, resolution):\n",
    "    \"\"\"\n",
    "    Calculate the boundaries of the field in right ascension (RA) and declination (Dec).\n",
    "    \n",
    "    :param ra: Dataframe column containing the right ascension values.\n",
    "    :param dec: Dataframe column containing the declination values.\n",
    "    :param resolution: Resolution of the map in arcminutes.\n",
    "    :return: A dictionary containing the corners of the map {'ra_min', 'ra_max', 'dec_min', 'dec_max'}.\n",
    "    \"\"\"\n",
    "    # Calculate median RA and Dec\n",
    "    med_ra = np.median(ra)\n",
    "    med_dec = np.median(dec)\n",
    "    \n",
    "    # Calculate the range of RA and Dec values\n",
    "    ra_range = np.max(ra) - np.min(ra)\n",
    "    dec_range = np.max(dec) - np.min(dec)\n",
    "    \n",
    "    # Calculate the size of the field in degrees\n",
    "    ra_size = ra_range * np.cos(np.deg2rad(med_dec))  # Adjust for declination\n",
    "    dec_size = dec_range\n",
    "    \n",
    "    # Calculate RA and Dec extents and store in a dictionary\n",
    "    boundaries = {\n",
    "        'ra_min': med_ra - ra_size / 2,\n",
    "        'ra_max': med_ra + ra_size / 2,\n",
    "        'dec_min': med_dec - dec_size / 2,\n",
    "        'dec_max': med_dec + dec_size / 2\n",
    "    }\n",
    "    \n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle_ra_dec(shear_df):\n",
    "    \"\"\"\n",
    "    Shuffle the 'ra' and 'dec' columns of the input DataFrame together.\n",
    "    \n",
    "    :param shear_df: Input pandas DataFrame.\n",
    "    :return: A new pandas DataFrame with shuffled 'ra' and 'dec' columns.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    shuffled_df = shear_df.copy()\n",
    "\n",
    "    # Combine RA and DEC into pairs\n",
    "    #list?\n",
    "    ra_dec_pairs = list(zip(shuffled_df['ra'], shuffled_df['dec']))\n",
    "    \n",
    "    # Shuffle the pairs\n",
    "    random.shuffle(ra_dec_pairs)\n",
    "    \n",
    "    # Unzip the shuffled pairs back into RA and DEC\n",
    "    shuffled_ra, shuffled_dec = zip(*ra_dec_pairs)\n",
    "    \n",
    "    shuffled_df['ra'] = shuffled_ra\n",
    "    shuffled_df['dec'] = shuffled_dec\n",
    "\n",
    "    return shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_shear_dfs(og_shear_df, num_shuffles):\n",
    "    \"\"\"\n",
    "    Generate a list of multiple data frames with shuffled RA and DEC columns by calling the load and shuffle functions.\n",
    "    Return: A list of shuffled pandas DataFrames.\n",
    "    \"\"\"\n",
    "    # Load the original shear data\n",
    "    #shear_df = load_shear_data(shear_cat_path, ra_col, dec_col, g1_col, g2_col, weight_col)\n",
    "    \n",
    "    # List to store the shuffled data frames (not sure if a list of these data frames is the best format rn)\n",
    "    shuffled_dfs = []\n",
    "    \n",
    "    # Loop to generate multiple shuffled data frames\n",
    "    for i in range(num_shuffles):\n",
    "        shuffled_df = _shuffle_ra_dec(og_shear_df)\n",
    "        shuffled_dfs.append(shuffled_df)\n",
    "    \n",
    "    return shuffled_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shear_grid(ra, dec, g1, g2, weight, boundaries, resolution):\n",
    "    '''\n",
    "    Bin values of shear data according to position on the sky.\n",
    "    '''\n",
    "    ra_min, ra_max = boundaries['ra_min'], boundaries['ra_max']\n",
    "    dec_min, dec_max = boundaries['dec_min'], boundaries['dec_max']\n",
    "    \n",
    "    # Calculate number of pixels based on field size and resolution\n",
    "    npix_ra = int(np.ceil((ra_max - ra_min) * 60 / resolution))\n",
    "    npix_dec = int(np.ceil((dec_max - dec_min) * 60 / resolution))\n",
    "    \n",
    "    ra_bins = np.linspace(ra_min, ra_max, npix_ra + 1)\n",
    "    dec_bins = np.linspace(dec_min, dec_max, npix_dec + 1)\n",
    "    \n",
    "    # Digitize the RA and Dec to find bin indices\n",
    "    ra_idx = np.digitize(ra, ra_bins) - 1\n",
    "    dec_idx = np.digitize(dec, dec_bins) - 1\n",
    "    \n",
    "    # Filter out indices that are outside the grid boundaries\n",
    "    valid_mask = (ra_idx >= 0) & (ra_idx < npix_ra) & (dec_idx >= 0) & (dec_idx < npix_dec)\n",
    "    ra_idx = ra_idx[valid_mask]\n",
    "    dec_idx = dec_idx[valid_mask]\n",
    "    g1 = g1[valid_mask]\n",
    "    g2 = g2[valid_mask]\n",
    "    weight = weight[valid_mask]\n",
    "    \n",
    "    # Initialize the grids\n",
    "    g1_grid = np.zeros((npix_dec, npix_ra))\n",
    "    g2_grid = np.zeros((npix_dec, npix_ra))\n",
    "    weight_grid = np.zeros((npix_dec, npix_ra))\n",
    "    \n",
    "    # Accumulate weighted values using np.add.at\n",
    "    np.add.at(g1_grid, (dec_idx, ra_idx), g1 * weight)\n",
    "    np.add.at(g2_grid, (dec_idx, ra_idx), g2 * weight)\n",
    "    np.add.at(weight_grid, (dec_idx, ra_idx), weight)\n",
    "    \n",
    "    # Normalize the grid by the total weight in each bin (weighted average)\n",
    "    #try with commented out \n",
    "    nonzero_weight_mask = weight_grid != 0\n",
    "    g1_grid[nonzero_weight_mask] /= weight_grid[nonzero_weight_mask]\n",
    "    g2_grid[nonzero_weight_mask] /= weight_grid[nonzero_weight_mask]\n",
    "    \n",
    "    return g1_grid, g2_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shear_grids_for_shuffled_dfs(list_of_dfs): \n",
    "    grid_list = []\n",
    "    for shear_df in list_of_dfs: \n",
    "        g1map, g2map = create_shear_grid(shear_df['ra'], \n",
    "                                           shear_df['dec'], \n",
    "                                           shear_df['g1'],\n",
    "                                           shear_df['g2'], \n",
    "                                           shear_df['weight'], \n",
    "                                           boundaries=boundaries,\n",
    "                                           resolution=config['resolution'])\n",
    "\n",
    "        grid_list.append((g1map, g2map))\n",
    "\n",
    "    return grid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_inversion(g1_grid, g2_grid):\n",
    "    \"\"\"\n",
    "    Perform the Kaiser-Squires inversion to obtain both E-mode and B-mode convergence maps from shear components.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input grids\n",
    "    npix_dec, npix_ra = g1_grid.shape\n",
    "\n",
    "    # Fourier transform the shear components\n",
    "    g1_hat = np.fft.fft2(g1_grid)\n",
    "    g2_hat = np.fft.fft2(g2_grid)\n",
    "\n",
    "    # Create a grid of wave numbers\n",
    "    k1, k2 = np.meshgrid(np.fft.fftfreq(npix_ra), np.fft.fftfreq(npix_dec))\n",
    "    k_squared = k1**2 + k2**2\n",
    "\n",
    "    # Avoid division by zero by replacing zero values with a small number\n",
    "    k_squared = np.where(k_squared == 0, np.finfo(float).eps, k_squared)\n",
    "\n",
    "    # Kaiser-Squires inversion in Fourier space\n",
    "    kappa_e_hat = (1 / k_squared) * ((k1**2 - k2**2) * g1_hat + 2 * k1 * k2 * g2_hat)\n",
    "    kappa_b_hat = (1 / k_squared) * ((k1**2 - k2**2) * g2_hat - 2 * k1 * k2 * g1_hat)\n",
    "\n",
    "    # Inverse Fourier transform to get the convergence maps\n",
    "    kappa_e_grid = np.real(np.fft.ifft2(kappa_e_hat))\n",
    "    kappa_b_grid = np.real(np.fft.ifft2(kappa_b_hat))\n",
    "\n",
    "    return kappa_e_grid, kappa_b_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_inversion_list(grid_list):\n",
    "    \"\"\"\n",
    "    Iterate through a list of (g1map, g2map) pairs and return a list of kappa_e values.\n",
    "    \n",
    "    Parameters:\n",
    "    shear_maps : list of tuples\n",
    "        A list where each element is a tuple of (g1map, g2map)\n",
    "        \n",
    "    Returns:\n",
    "    kappa_e_list : list\n",
    "        A list containing the kappa_e maps for each (g1map, g2map) pair.\n",
    "    \"\"\"\n",
    "    kappa_e_list = []\n",
    "    kappa_b_list = []\n",
    "    \n",
    "    for g1map, g2map in grid_list:\n",
    "        # Call the ks_inversion function for each pair\n",
    "        kappa_e, kappa_b = ks_inversion(g1map, -g2map)  # We only care about kappa_e\n",
    "        kappa_e_list.append(kappa_e)\n",
    "        kappa_b_list.append(kappa_b)\n",
    "    \n",
    "    return kappa_e_list, kappa_b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(convergence, boundaries, config):\n",
    "    \"\"\"\n",
    "    Make plot of convergence map and save to file using information passed\n",
    "    in run configuration file. \n",
    "\n",
    "    Arguments\n",
    "        convergence: XXX raw convergence map XXX\n",
    "        boundaries: XXX RA/Dec axis limits for plot, set in XXX\n",
    "        config: overall run configuration file\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Embiggen font sizes, tick marks, etc.\n",
    "    fontsize = 15\n",
    "    plt.rcParams.update({'axes.linewidth': 1.3})\n",
    "    plt.rcParams.update({'xtick.labelsize': fontsize})\n",
    "    plt.rcParams.update({'ytick.labelsize': fontsize})\n",
    "    plt.rcParams.update({'xtick.major.size': 8})\n",
    "    plt.rcParams.update({'xtick.major.width': 1.3})\n",
    "    plt.rcParams.update({'xtick.minor.visible': True})\n",
    "    plt.rcParams.update({'xtick.minor.width': 1.})\n",
    "    plt.rcParams.update({'xtick.minor.size': 6})\n",
    "    plt.rcParams.update({'xtick.direction': 'in'})\n",
    "    plt.rcParams.update({'ytick.major.width': 1.3})\n",
    "    plt.rcParams.update({'ytick.major.size': 8})\n",
    "    plt.rcParams.update({'ytick.minor.visible': True})\n",
    "    plt.rcParams.update({'ytick.minor.width': 1.})\n",
    "    plt.rcParams.update({'ytick.minor.size':6})\n",
    "    plt.rcParams.update({'ytick.direction':'in'})\n",
    "    plt.rcParams.update({'axes.labelsize': fontsize})\n",
    "    plt.rcParams.update({'axes.titlesize': fontsize})\n",
    "\n",
    "    \n",
    "    # Apply Gaussian filter -- is this the right place to do it?\n",
    "    # We are planning on implementing other filters at some point, right?\n",
    "    filtered_convergence = gaussian_filter(convergence, config['gaussian_kernel'])\n",
    "\n",
    "    # Make the plot!\n",
    "    fig, ax = plt.subplots(\n",
    "        nrows=1, ncols=1, figsize=config['figsize'], tight_layout=True\n",
    "    )\n",
    "    \n",
    "    im = ax.imshow(\n",
    "        filtered_convergence[:, ::-1], \n",
    "        cmap=config['cmap'],\n",
    "        vmax=config['vmax'], \n",
    "        vmin=config['vmin'],\n",
    "        extent=[boundaries['ra_max'], \n",
    "                    boundaries['ra_min'], \n",
    "                    boundaries['dec_min'], \n",
    "                    boundaries['dec_max']],\n",
    "        origin='lower' # Sets the origin to bottom left to match the RA/DEC convention\n",
    "    )  \n",
    "\n",
    "    ax.set_xlabel(config['xlabel'])\n",
    "    ax.set_ylabel(config['ylabel'])\n",
    "    ax.set_title(config['plot_title'])\n",
    "\n",
    "    # Is there a better way to force something to be a boolean?\n",
    "    if config['gridlines'] == True:\n",
    "        ax.grid(color='black')\n",
    "\n",
    "    # Add colorbar; turn off minor axes first\n",
    "    plt.rcParams.update({'ytick.minor.visible': False})\n",
    "    plt.rcParams.update({'xtick.minor.visible': False})\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.07)\n",
    "    fig.colorbar(im, cax=cax)\n",
    "\n",
    "    # Save to file and exit, redoing tight_layout b/c sometimes figure gets cut off \n",
    "    fig.tight_layout() \n",
    "    plt.show()\n",
    "    fig.savefig(config['output_path'])\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shear_df = load_shear_data(config['input_path'], \n",
    "                                          config['ra_col'], \n",
    "                                          config['dec_col'], \n",
    "                                          config['g1_col'], \n",
    "                                          config['g2_col'], \n",
    "                                          config['weight_col'])\n",
    "print(shear_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dfs = generate_multiple_shear_dfs(shear_df, 100)\n",
    "first_df = shuffled_dfs[0]\n",
    "boundaries = calculate_field_boundaries(shear_df['ra'], shear_df['dec'], config['resolution'])\n",
    "#for shear_df in shuffled_dfs:\n",
    "    #print(shear_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_g2_map_list = shear_grids_for_shuffled_dfs(shuffled_dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuff_kappa_e_list, shuff_kappa_b_list = ks_inversion_list(g1_g2_map_list)\n",
    "print(shuff_kappa_e_list[2])\n",
    "print(shuff_kappa_b_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacks all the maps into a 3D array (axis = 0 is the depth across all the maps)\n",
    "kappa_e_stack = np.stack(shuff_kappa_e_list, axis = 0)\n",
    "kappa_b_stack = np.stack(shuff_kappa_b_list, axis = 0)\n",
    "#takes the variance across each map for each pixel \n",
    "variance_map_e = np.var(kappa_e_stack, axis = 0)\n",
    "variance_map_b = np.var(kappa_b_stack, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, computing the oringinal kappa map\n",
    "g1map_og, g2map_og = create_shear_grid(shear_df['ra'], \n",
    "                                       shear_df['dec'], \n",
    "                                       shear_df['g1'],\n",
    "                                       shear_df['g2'], \n",
    "                                       shear_df['weight'], \n",
    "                                       boundaries=boundaries,\n",
    "                                       resolution=config['resolution'])\n",
    "og_kappa_e, og_kappa_b = ks_inversion(g1map_og, -g2map_og)\n",
    "std = np.sqrt(variance_map_e)\n",
    "print(std)\n",
    "print(og_kappa_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(kappa_e_stack, axis = 0)\n",
    "\n",
    "plt.hist(og_kappa_e.ravel(), bins = 100, label = \"SIGNAL\")\n",
    "\n",
    "\n",
    "plt.hist(mean.ravel(), bins=100, label = \"MEAN\")\n",
    "print(np.mean(mean.ravel()))\n",
    "\n",
    "variance = np.sqrt(variance_map_e)\n",
    "plt.hist(variance.ravel(), bins=100, label = \"STD\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_signal = np.median(og_kappa_e)\n",
    "signal_to_noise = og_kappa_e / std\n",
    "print(signal_to_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(signal_to_noise, boundaries, config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
